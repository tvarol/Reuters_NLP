{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to make text classification multilingual is to develop multilingual word embeddings. With this technique, embeddings for every language exist in the same vector space, and maintain the property that words with similar meanings (regardless of language) are close together in vector space. For example, the words futbol in Turkish and soccer in English would appear very close together in the embedding space because they mean the same thing in different languages.\n",
    "\n",
    "__Goal:__ Inducing word translations using only monolingual corpora for two languages. Separate embeddings are trained for each language and a mapping is learned though an adversarial objective, along with an orthogonality constraint on the most frequent words.\n",
    "\n",
    "Mapping can be obtained by using MUSE package:\n",
    "\n",
    "First download the monolingual word embeddings:\n",
    "\n",
    "curl -Lo data/wiki.en.vec https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "\n",
    "curl -Lo data/wiki.fr.vec https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "Then run the mapping:\n",
    "python supervised.py --src_lang en --tgt_lang fr --src_emb data/wiki.en.vec --tgt_emb data/wiki.fr.vec --n_refinement 5 --dico_train default --cuda False\n",
    "\n",
    "As an output (best_mapping.pth), I got the 300-dimension matrix, which is a translation matrix. Thus, given a point in a vector space for a word in English, and multiplying it with the translation matrix will give the point in the vector space for the corresponding word in French.\n",
    "\n",
    "## Ideas to implement it:\n",
    "### First idea: Using monolingual embeddings\n",
    "- Get the representation of the document in English\n",
    "- Find each token from the document, inside the monolingual embedding file (wiki.en.vec) and get the coefficient for that.\n",
    "- Create matrix with vocabulary size and 300 dimensions with all zeros\n",
    "- Looping over the words of a given document, for each iteration, (or for each word) set the matrix element (created with all zeros) to the coefficient obtained from wiki.en.vec as described in step 2.\n",
    "- At the end, multiply the created matrix with the translation matrix from MUSE package (created as explained at the beginning).\n",
    "- Use that matrix as weights in the Embedding layer.\n",
    "- The final prediction can be found in the vector space given for French.\n",
    "\n",
    "\n",
    "### Second idea: Using en-fr dictionary\n",
    "- Get the representation of the document in English\n",
    "- For each English word, look at the en-fr dictionary and find the corresponding French word\n",
    "- Train documents will be the English words obtained from the given document, whereas labels will be the French words. \n",
    "- The translation matrix will be fed to the Embedding layer in Keras as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import torch \n",
    "\n",
    "emb_en_fr = torch.load(\"/Users/tulinvarol/MUSE/dumped/debug/x7og08cwhk/best_mapping.pth\")\n",
    "dict_en_fr = open(\"/Users/tulinvarol/MUSE/data/crosslingual/dictionaries/en-fr.txt\")\n",
    "\n",
    "word_en = []\n",
    "word_fr = []\n",
    "embeddings_index = {}\n",
    "for line in dict_en_fr:\n",
    "    values = line.split(' ')\n",
    "    word_en.append(values[0])\n",
    "    word_fr.append(values[1].strip())\n",
    "    embeddings_index\n",
    "dict_en_fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
