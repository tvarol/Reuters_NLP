{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word embeddings provide a dense representation of words and their relative meanings.\n",
    "\n",
    "They are an improvement over sparse representations used in simpler bag of word model representations.\n",
    "These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n",
    "\n",
    "Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n",
    "\n",
    "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "The position of a word in the learned vector space is referred to as its embedding.\n",
    "\n",
    "\n",
    "## Keras Embedding Layer\n",
    "\n",
    "Keras offers an Embedding layer that can be used for neural networks on text data.\n",
    "It requires that the input data be integer encoded, so that each word is represented by a unique integer. \n",
    "\n",
    "In this project, we'll show how a word embedding is learnt while fitting a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train and Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define documents\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "train_docs = []\n",
    "test_docs = []\n",
    "train_id = []\n",
    "test_id = []\n",
    " \n",
    "for id in reuters.fileids():\n",
    "    if id.startswith(\"train\"):\n",
    "        train_docs.append(reuters.raw(id))\n",
    "        train_id.append(id)\n",
    "    else:\n",
    "        test_docs.append(reuters.raw(id))\n",
    "        test_id.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Cleaning and Representation\n",
    "\n",
    "Using Tokenizer from Keras:\n",
    "\n",
    "* Removing punctutations, putting words in lower case, tokenizing.\n",
    "* The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 7769. (This number is chosen not because we have 7769 documents but because the longest document in the corpus has around 7000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769, 7769)\n",
      "[5963  526  904 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=25000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "encoded_train_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "encoded_test_docs=tokenizer.texts_to_sequences(test_docs)\n",
    "#word_index = tokenizer.word_index\n",
    "\n",
    "# pad documents to a max length of 7769 words\n",
    "max_length = 7769\n",
    "padded_train_docs = pad_sequences(encoded_train_docs, maxlen=max_length, padding='post')\n",
    "padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
    "print(padded_train_docs.shape)\n",
    "print(padded_train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7769, 90)\n",
      "[u'acq', u'alum', u'barley', u'bop', u'carcass', u'castor-oil', u'cocoa', u'coconut', u'coconut-oil', u'coffee', u'copper', u'copra-cake', u'corn', u'cotton', u'cotton-oil', u'cpi', u'cpu', u'crude', u'dfl', u'dlr', u'dmk', u'earn', u'fuel', u'gas', u'gnp', u'gold', u'grain', u'groundnut', u'groundnut-oil', u'heat', u'hog', u'housing', u'income', u'instal-debt', u'interest', u'ipi', u'iron-steel', u'jet', u'jobs', u'l-cattle', u'lead', u'lei', u'lin-oil', u'livestock', u'lumber', u'meal-feed', u'money-fx', u'money-supply', u'naphtha', u'nat-gas', u'nickel', u'nkr', u'nzdlr', u'oat', u'oilseed', u'orange', u'palladium', u'palm-oil', u'palmkernel', u'pet-chem', u'platinum', u'potato', u'propane', u'rand', u'rape-oil', u'rapeseed', u'reserves', u'retail', u'rice', u'rubber', u'rye', u'ship', u'silver', u'sorghum', u'soy-meal', u'soy-oil', u'soybean', u'strategic-metal', u'sugar', u'sun-meal', u'sun-oil', u'sunseed', u'tea', u'tin', u'trade', u'veg-oil', u'wheat', u'wpi', u'yen', u'zinc']\n",
      "earn\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Transform multilabel labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform([reuters.categories(id)\n",
    "                                  for id in train_id])\n",
    "test_labels = mlb.transform([reuters.categories(id)\n",
    "                             for id in test_id])\n",
    "\n",
    "print(train_labels.shape)\n",
    "print(reuters.categories())\n",
    "print(reuters.categories()[21])\n",
    "print(test_labels[60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying 3 arguments:\n",
    "\n",
    "__input_dim:__ This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "__output_dim:__ This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. \n",
    "\n",
    "__input_length:__ This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "The output from the Embedding layer will be 7769 vectors of 30 dimensions each. We flatten this to a one (7769*30)-dimensions element vector to pass on to the Dense output layer.\n",
    "\n",
    "(The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document). In order to connect a Dense layer directly to an Embedding layer, we first need to flatten the 2D output matrix to a 1D vector using the Flatten layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 7769, 30)          750000    \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 233070)            0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 90)                20976390  \n",
      "=================================================================\n",
      "Total params: 21,726,390\n",
      "Trainable params: 21,726,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/8\n",
      "7769/7769 [==============================] - 134s 17ms/step - loss: 0.0457 - acc: 0.9880\n",
      "Epoch 2/8\n",
      "7769/7769 [==============================] - 147s 19ms/step - loss: 0.0221 - acc: 0.9939\n",
      "Epoch 3/8\n",
      "7769/7769 [==============================] - 147s 19ms/step - loss: 0.0115 - acc: 0.9965\n",
      "Epoch 4/8\n",
      "7769/7769 [==============================] - 141s 18ms/step - loss: 0.0060 - acc: 0.9983\n",
      "Epoch 5/8\n",
      "7769/7769 [==============================] - 144s 19ms/step - loss: 0.0036 - acc: 0.9991\n",
      "Epoch 6/8\n",
      "7769/7769 [==============================] - 100s 13ms/step - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 7/8\n",
      "7769/7769 [==============================] - 86s 11ms/step - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 8/8\n",
      "7769/7769 [==============================] - 80s 10ms/step - loss: 0.0013 - acc: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x162124c10>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 30, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(90, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_train_docs, train_labels, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.982980\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_train_docs, train_labels, verbose=0)\n",
    "#loss, accuracy = model.evaluate(padded_test_docs, test_labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "earn\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(padded_test_docs)\n",
    "\n",
    "preds[preds>=0.5] = 1\n",
    "preds[preds<0.5] = 0\n",
    "\n",
    "pred_new = preds[60]\n",
    "print(pred_new)\n",
    "idx = np.where(pred_new>=1)\n",
    "for val in idx[0]:\n",
    "    print(reuters.categories()[int(val)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
